import os
import time
import streamlit as st
from pypdf import PdfReader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_groq import ChatGroq

# TODO: Write relevant menu items
st.set_page_config(
    page_title = "Document Assistant",
    page_icon = "ðŸ“„",
    menu_items = {
        'Get Help': 'https://www.extremelycoolapp.com/help',
        'Report a bug': "https://www.extremelycoolapp.com/bug",
        'About': "# This is a header. This is an *extremely* cool app!"
    }
)

# Read the API key from a file
with open("C:\\Programming\\doc reader chatbot\\groq.txt", "r", encoding="utf-8") as apiKeyFile:
    apiKey = apiKeyFile.read().strip()
# Set the API key as an environment variable
os.environ["GROQ_API_KEY"] = apiKey

# TODO: Read the following to try fixing the spinner issue
# https://github.com/streamlit/streamlit/issues/3838

def main():
    st.title("Welcome To Document Assistant")
    st.subheader("How can I help you?")

    option = st.selectbox(
        "What is the extension of the file in question?",
        ("pdf", "md", "txt"),
    )
    file = st.file_uploader("Upload your file", type = option)

    if file:
        text = ""
        match option:
            case "pdf":
                with st.spinner("Processing file..."):
                    pdf = PdfReader(file)
                    for page in pdf.pages:
                        text += page.extract_text()

            case "txt" | "md":
                text = file.getvalue().decode("utf-8")

        query = st.text_input("Ask the AI a question about your file", placeholder = "What is the premise of the story?")
        confirm = st.button("Confirm")

        start = time.time()

        if confirm:
            cancel = st.button('Cancel')
            if cancel:
                st.stop()
            elif query:
                with st.spinner("Understanding the file content, this may take a few minutes..."):
                    db = processText(text)
                    retriever = db.as_retriever(search_type = "similarity")
                    results = retriever.invoke(query)
                    response = analyzeResults(results, query)

                st.header("AI response")
                st.markdown(response.content)

                end = time.time()
                st.markdown("Time taken: " + str(round(end - start, 2)) + " seconds")

def processText(text):
    """
    Split the given text into chunks, convert them into embeddings, and save them into a vector db.

    @Params:
        text (str): The input text to be processed.

    @Returns:
        Chroma: The database containing the chunks of text converted into embeddings.
    """

    # Split the text into chunks using Langchain's RecursiveCharacterTextSplitter
    splitter = RecursiveCharacterTextSplitter(
        chunk_size = 1024,
        chunk_overlap = 50,
)
    chunks = splitter.split_text(text)

    # Convert the chunks of text into embeddings to form a database
    embeddings = HuggingFaceEmbeddings()
    db = Chroma.from_texts(chunks, embeddings)

    return db

def analyzeResults(results, query):
    """
    Formats the results of the retrieval into as context, then generates a prompt using a template, the aforementioned context, and the user query. The prompt is then used for the language model to generate a human-like answer to the query based on the given context.

    @Params:
        results (list): A list of documents containing the search results.
        query (str): The query to be answered.

    @Returns:
        BaseMessage: The response generated by the language model.
    """

    context = "\n\n---\n\n".join([doc.page_content for doc in results])

    template = """
    Answer the query based only on the given context:
    {context}
    ---
    Answer the query based on the above context: {query}
    """

    prompt = template.format(context = context, query = query)

    llm = ChatGroq()
    response = llm.invoke(prompt)

    return response

main()
